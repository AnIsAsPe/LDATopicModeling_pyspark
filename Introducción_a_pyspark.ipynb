{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introducción a pyspark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1aWoa5uIU9yB8G1sN1aCvFeI_FN6tnsI2",
      "authorship_tag": "ABX9TyOGYlT92+1pKmZz8UPegmDj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnIsAsPe/LDATopicModeling_pyspark/blob/main/Introducci%C3%B3n_a_pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCbQIBhpmx_y"
      },
      "source": [
        "\n",
        "\n",
        "![image.png](https://spark.apache.org/docs/1.1.1/img/cluster-overview.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hi6OGDnO2f2"
      },
      "source": [
        "# Instalación de PySpark en Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BbA2UaqQOax",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc40379a-5e0f-4e26-f9c1-225efdf27706"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/db/e18cfd78e408de957821ec5ca56de1250645b05f8523d169803d8df35a64/pyspark-3.1.2.tar.gz (212.4MB)\n",
            "\u001b[K     |████████████████████████████████| 212.4MB 71kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 15.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=31996be7abdeb774f201dd129d8aca50cbe245fcceeaaeec7dfe886de63879ec\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/1b/2c/30f43be2627857ab80062bef1527c0128f7b4070b6b2d02139\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pf3kS3f2IcTW"
      },
      "source": [
        "# Spark session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-Mq8KI6xjd-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "eaadd404-2cdc-4983-d9de-81e0d4dac616"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"Prueba_spark_colab\").getOrCreate()\n",
        "spark"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://e413f007c213:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Prueba_spark_colab</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f36cc6c32d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TxAujHu236V"
      },
      "source": [
        "# Spark Context"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhBpsCzr296y"
      },
      "source": [
        "# Crear SparkContext para conectar con el cluster, antes es necesario tener SparkSession\n",
        "from pyspark import SparkConf\n",
        "from pyspark import SparkContext\n",
        "import numpy as np\n",
        "\n",
        "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[4]\"))\n",
        "\n",
        "# \"the master\" es la computadora conectada con el resto de las computadoras en el cluster que\n",
        "#  administra la división y transformación de los datos"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0b28MH2rE84"
      },
      "source": [
        "# Resilent Distributed Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYFcA7v-rMAc"
      },
      "source": [
        "## Crear un objeto RDD paralelizazando una colección"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9osfnc07J7G1"
      },
      "source": [
        "a_np = np.random.randint(0,100,20)\n",
        "a_rdd = sc.parallelize(a_np)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xx-Juh62JfEp",
        "outputId": "fb132418-e5b4-4445-b7f8-446bc9d7004b"
      },
      "source": [
        "print(type(a_np))\n",
        "print(type(a_rdd))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "<class 'pyspark.rdd.RDD'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOPWDtGiJr7m",
        "outputId": "f2cdc94e-c3fd-4f49-a030-ea22b58840f5"
      },
      "source": [
        "a_rdd.collect()  # regresa los elementos distribuidos "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[54, 78, 49, 15, 43, 93, 92, 27, 35, 60, 84, 44, 15, 29, 59, 36, 8, 23, 50, 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7uz-hhTJzyl",
        "outputId": "a594cdb7-2088-4032-b26c-f30caf8dae27"
      },
      "source": [
        "a_rdd.glom().collect()  # con glom podemos ver como se hicieron las particiones"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[54, 78, 49, 15, 43, 93, 92, 27, 35, 60],\n",
              " [84, 44, 15, 29, 59, 36, 8, 23, 50, 0]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5HK_MCeLIbe",
        "outputId": "e796c7c3-6df4-4352-9ec7-0ae2ab2236f0"
      },
      "source": [
        "sc.stop()\n",
        "sc=SparkContext(master=\"local[2]\")\n",
        "a_rdd = sc.parallelize(a_np)\n",
        "a_rdd.collect()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[54, 78, 49, 15, 43, 93, 92, 27, 35, 60, 84, 44, 15, 29, 59, 36, 8, 23, 50, 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAnmLnZ9rauu"
      },
      "source": [
        "## Crear un objeto RDD a partir de datos externos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_A7h6Ff9rsvM"
      },
      "source": [
        "texto = sc.textFile(\"/content/drive/MyDrive/Datos/gabriel_garcia_marquez_cien_annos_soledad.txt\")\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ieCOr2rtLMv",
        "outputId": "1b8a0dbb-cb64-451c-94d5-ca3dee76fecf"
      },
      "source": [
        "type(texto)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.rdd.RDD"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8s3PdaXItv90",
        "outputId": "3968d57a-149d-4048-bc49-07e524cf8e31"
      },
      "source": [
        "texto.map(lambda s: len(s)).reduce(lambda a, b: a + b) # cantidad de caracteres"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "814791"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQhe7id2fwnG"
      },
      "source": [
        "# Operaciones en RDD\n",
        "- Transformaciones (Map)\n",
        "- Acciones (Reduce)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOgVWsFCgwT3"
      },
      "source": [
        "### Transformaciones (Map"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9tqK1n3Is2k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e5ff948-2cad-4398-a041-df66952037ab"
      },
      "source": [
        "#map function\n",
        "sc.parallelize([3,4,5]).map(lambda x: range(1,x)).collect()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[range(1, 3), range(1, 4), range(1, 5)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvtoyNXvf8Y3",
        "outputId": "0ab40c24-c90f-476d-c1b0-6cb1dcdf11dd"
      },
      "source": [
        "#flatmap example.So it creates output like map function but it flattens the output in a list\n",
        "sc.parallelize([3,4,5]).flatMap(lambda x: [x, x*x]).collect()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 9, 4, 16, 5, 25]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVGAulI2louu",
        "outputId": "507b3e15-5b05-442c-8907-8e16cf0d52dc"
      },
      "source": [
        "#mapping con funciones regulares\n",
        "def square_if_odd(x):\n",
        "    \"\"\"\n",
        "    Si el numero es non, regresa el cuadrado, los pares en cambio\n",
        "    no sufren transformación\n",
        "    \"\"\"\n",
        "    if x%2==1:\n",
        "        return x*x\n",
        "    else:\n",
        "        return x\n",
        "\n",
        "numeros = sc.parallelize(np.arange(20))\n",
        "numeros.map(square_if_odd).collect()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 2, 9, 4, 25, 6, 49, 8, 81, 10, 121, 12, 169, 14, 225, 16, 289, 18, 361]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m27VEr7FgzKx"
      },
      "source": [
        "###  Acciones (Reduce)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsBpkk4thCgg"
      },
      "source": [
        "numbers = sc.parallelize([1, 4, 6, 2, 9, 10])\n",
        "sum = numbers.reduce(lambda a,b : a+b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTdU9RrnLk13"
      },
      "source": [
        "numbers.count() #conotar los elementos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NszV1--LoMW"
      },
      "source": [
        "numbers.first()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ws_K6B6ALrg-"
      },
      "source": [
        "numbers.take(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yh3raTh6kIv5"
      },
      "source": [
        "#Encontrar el elemento máximo con reduce\n",
        "numbers.reduce(lambda x,y: x if x > y else y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1I5jLac6kx_-"
      },
      "source": [
        "#Filtros: ejemplo devuelve numeros positivos divisibles entre 3\n",
        "numbers.filter(lambda x:x%3==0 and x>=0).collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XB1ReC1EkWkB",
        "outputId": "8ec114d4-de4b-4a48-becb-734852c7b619"
      },
      "source": [
        "\n",
        "words = 'MapReduce, GFS, Hadoop, HDFS, Spark'.split(',')\n",
        "\n",
        "wordRDD = sc.parallelize(words)\n",
        "\n",
        "wordRDD.reduce(lambda w,v: w if len(w) >len(v) else v)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'MapReduce'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KMr0uSvGlM2-",
        "outputId": "966e33e5-7b7c-455c-dee4-da95b59ad92e"
      },
      "source": [
        "#uso de funciones comunes utilizando reduce \n",
        "def largerThan(x,y):\n",
        "    \"\"\"\n",
        "    Regresa la palabra más larga\n",
        "    \"\"\"\n",
        "    if len(x)> len(y):\n",
        "        return x\n",
        "    else:\n",
        "        return y\n",
        "\n",
        "wordRDD.reduce(largerThan)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'MapReduce'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QwVM2L5mGOW"
      },
      "source": [
        "# Groupby"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojdQX2vKme4W",
        "outputId": "5f1f0bea-f99c-4dae-99f2-434865c765ef"
      },
      "source": [
        "result=numeros.groupBy(lambda x:x%2).collect()\n",
        "result"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, <pyspark.resultiterable.ResultIterable at 0x7f36cc541fd0>),\n",
              " (1, <pyspark.resultiterable.ResultIterable at 0x7f36cc565b10>)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVS8q0GTmK2U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eab97416-1065-4833-f1cb-400391f43189"
      },
      "source": [
        "sorted([(x, sorted(y)) for (x, y) in result])"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]),\n",
              " (1, [1, 3, 5, 7, 9, 11, 13, 15, 17, 19])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeERnQF7nizZ"
      },
      "source": [
        "## Lazy Evaluation \n",
        "Es una estrategia de spark para acelerar operaciones paralelizadas.\n",
        "Deja lista una secuencia de tareas paso por paso en una tarea pero retraza la ejecución hasta que es absolutamente necesaria.\n",
        "\n",
        "(ejemplo en https://dzone.com/articles/the-benefits-amp-examples-of-using-apache-spark-wi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9u3wWRacEzu"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "Documentación oficial de spark: \n",
        "\n",
        "* [RDD Prigramming Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html#initializing-spark)\n",
        "\n",
        "* [Cluster Mode Oberview](https://databricks.com/blog/2018/05/03/benchmarking-apache-spark-on-a-single-node-machine.html)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}