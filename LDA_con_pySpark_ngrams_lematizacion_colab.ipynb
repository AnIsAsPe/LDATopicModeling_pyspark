{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LDA_con_pySpark-ngrams lematizacion_colab.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1NMSndtR057_",
        "D0vVi8o6_aRS"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnIsAsPe/LDATopicModeling_pyspark/blob/main/LDA_con_pySpark_ngrams_lematizacion_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hi6OGDnO2f2"
      },
      "source": [
        "# Instalación de PySpark en Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6ulR4kizLvy",
        "outputId": "bd0f5c20-ab96-4f77-cc19-6b801804c810"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/db/e18cfd78e408de957821ec5ca56de1250645b05f8523d169803d8df35a64/pyspark-3.1.2.tar.gz (212.4MB)\n",
            "\u001b[K     |████████████████████████████████| 212.4MB 84kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 22.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=737463d20f8a73e32e41265b25ad5f4ed75e1d1c968ee395e27b11723bcbae8f\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/1b/2c/30f43be2627857ab80062bef1527c0128f7b4070b6b2d02139\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSAcTGc2x5zC",
        "outputId": "5e9bda14-2fc7-43cf-b992-90cd61a31061"
      },
      "source": [
        "import os\n",
        "os.cpu_count()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "An553nK2x5zD",
        "outputId": "efb8967f-7834-41b8-f2d3-c51295e9ebd4"
      },
      "source": [
        "!echo $(($(getconf _PHYS_PAGES) * $(getconf PAGE_SIZE) / (1024 * 1024)))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12993\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5wmR_zBxhrD"
      },
      "source": [
        "## Crear Sesión Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-Mq8KI6xjd-"
      },
      "source": [
        "# Crear una sesión de spark\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "0-c_EzRxxxjN",
        "outputId": "d7767e2d-cf0e-42ee-bf2f-66f3c2dacde7"
      },
      "source": [
        "spark"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://c28bb4cb9cd2:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>colab</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f9d155c3050>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TxAujHu236V"
      },
      "source": [
        "## Importar bibliotecas "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhBpsCzr296y",
        "outputId": "fca928a6-9ebb-4644-e8aa-fee139603a9a"
      },
      "source": [
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import udf, concat, split, col\n",
        "from pyspark.ml.feature import RegexTokenizer, NGram, VectorAssembler, CountVectorizer, IDF\n",
        "from pyspark.ml.clustering import LDA, LocalLDAModel\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml import PipelineModel\n",
        "from pyspark.ml.feature import  CountVectorizerModel\n",
        "from pyspark.ml.clustering import LocalLDAModel\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')  #WordNetLemmatizer\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koJQrSJL095M"
      },
      "source": [
        "# Leer los datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2liL4jWzMgc",
        "outputId": "9ee81354-c2bc-4e14-9bd9-79816483a413"
      },
      "source": [
        "df = spark.read.csv(\"/content/drive/MyDrive/Datos/abcnews-date-text.csv\",header=True)\n",
        "\n",
        "print('Cantidad de renglones: ', df.count())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cantidad de renglones:  1226258\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeCfIBYb3j4k",
        "outputId": "2f71aae5-5801-405d-f29a-0af81fe80bf4"
      },
      "source": [
        "df.printSchema() # explorar la estructura el dataframe"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- publish_date: string (nullable = true)\n",
            " |-- headline_text: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqEhSGci5HzE",
        "outputId": "256b751d-f6ac-4a75-bac2-5c4ea558b7d5"
      },
      "source": [
        "df.head(3)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(publish_date='20030219', headline_text='aba decides against community broadcasting licence'),\n",
              " Row(publish_date='20030219', headline_text='act fire witnesses must be aware of defamation'),\n",
              " Row(publish_date='20030219', headline_text='a g calls for infrastructure protection summit')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdpu1WZc5Tbk",
        "outputId": "2f684b70-3e06-4a35-f01d-4e303c84d2b6"
      },
      "source": [
        "df.show(10) # default 20 renglones"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+--------------------+\n",
            "|publish_date|       headline_text|\n",
            "+------------+--------------------+\n",
            "|    20030219|aba decides again...|\n",
            "|    20030219|act fire witnesse...|\n",
            "|    20030219|a g calls for inf...|\n",
            "|    20030219|air nz staff in a...|\n",
            "|    20030219|air nz strike to ...|\n",
            "|    20030219|ambitious olsson ...|\n",
            "|    20030219|antic delighted w...|\n",
            "|    20030219|aussie qualifier ...|\n",
            "|    20030219|aust addresses un...|\n",
            "|    20030219|australia is lock...|\n",
            "+------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "araSDXeh0gKM",
        "outputId": "9da80f0d-27df-41a5-8c66-ef25cda2b19e"
      },
      "source": [
        "df.select('headline_text').take(1)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(headline_text='aba decides against community broadcasting licence')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sjr90qIi0M-L",
        "outputId": "e5462087-21b0-4186-fb54-b07c3011851f"
      },
      "source": [
        "# raw text of the first entry \n",
        "df.select('headline_text').head(1)[0][0]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'aba decides against community broadcasting licence'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aUQo7g71dsE",
        "outputId": "d9706f76-8e9f-49a7-bf07-dee56262b848"
      },
      "source": [
        "type(df)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.dataframe.DataFrame"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2RCzNuN0579"
      },
      "source": [
        "Queremos un inice consecutivo, para ello vamos a utilizar solo la columna 'hedline_text' y usando rdd creamos el indice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIkeM7R51RHi"
      },
      "source": [
        "texts = df.rdd.map(lambda x: x['headline_text'])\n",
        "headlines=texts.zipWithIndex( )                    "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Um4zlKJa057-"
      },
      "source": [
        "from pyspark.sql import SQLContext\n",
        "sqlContext = SQLContext(spark)\n",
        "#Creating dataframe\n",
        "data = sqlContext.createDataFrame(headlines, [\"headlines\",'index'])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYyebrRS2hdQ"
      },
      "source": [
        "# Preprocesar Texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVEOl5cJ057-"
      },
      "source": [
        "## Normalizar y tokenizar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTrWBzku057-"
      },
      "source": [
        "removePunct = udf(lambda s: s.strip().lower(), StringType())\n",
        "\n",
        "data_norm = data.withColumn(\"text\", removePunct(data.headlines))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0R3njG0057_"
      },
      "source": [
        "# tokenize \n",
        "tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\",\n",
        "                           gaps=True, pattern=r'\\s+', minTokenLength=4)\n",
        "df_tokens = tokenizer.transform(data_norm)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CClFRuE057_",
        "outputId": "e1ed05fb-965e-4182-edd3-770765db8e90"
      },
      "source": [
        "df_tokens.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-----+--------------------+--------------------+\n",
            "|           headlines|index|                text|               words|\n",
            "+--------------------+-----+--------------------+--------------------+\n",
            "|aba decides again...|    0|aba decides again...|[decides, against...|\n",
            "|act fire witnesse...|    1|act fire witnesse...|[fire, witnesses,...|\n",
            "|a g calls for inf...|    2|a g calls for inf...|[calls, infrastru...|\n",
            "|air nz staff in a...|    3|air nz staff in a...|[staff, aust, str...|\n",
            "|air nz strike to ...|    4|air nz strike to ...|[strike, affect, ...|\n",
            "|ambitious olsson ...|    5|ambitious olsson ...|[ambitious, olsso...|\n",
            "|antic delighted w...|    6|antic delighted w...|[antic, delighted...|\n",
            "|aussie qualifier ...|    7|aussie qualifier ...|[aussie, qualifie...|\n",
            "|aust addresses un...|    8|aust addresses un...|[aust, addresses,...|\n",
            "|australia is lock...|    9|australia is lock...|[australia, locke...|\n",
            "|australia to cont...|   10|australia to cont...|[australia, contr...|\n",
            "|barca take record...|   11|barca take record...|[barca, take, rec...|\n",
            "|bathhouse plans m...|   12|bathhouse plans m...|[bathhouse, plans...|\n",
            "|big hopes for lau...|   13|big hopes for lau...|[hopes, launcesto...|\n",
            "|big plan to boost...|   14|big plan to boost...|[plan, boost, par...|\n",
            "|blizzard buries u...|   15|blizzard buries u...|[blizzard, buries...|\n",
            "|brigadier dismiss...|   16|brigadier dismiss...|[brigadier, dismi...|\n",
            "|british combat tr...|   17|british combat tr...|[british, combat,...|\n",
            "|bryant leads lake...|   18|bryant leads lake...|[bryant, leads, l...|\n",
            "|bushfire victims ...|   19|bushfire victims ...|[bushfire, victim...|\n",
            "+--------------------+-----+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NMSndtR057_"
      },
      "source": [
        "## Removing stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKKKK5F1058A"
      },
      "source": [
        "stopwords = stopwords.words(\"english\")\n",
        "removeStop=udf(lambda word: [x for x in word if x not in stopwords])\n",
        "df_tokens=df_tokens.withColumn('noStopWords',removeStop(df_tokens['words']))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCqO4X-2058A",
        "outputId": "a183550c-e52f-4f8b-9398-ee374c5cbf2e"
      },
      "source": [
        "df_tokens.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-----+--------------------+--------------------+--------------------+\n",
            "|           headlines|index|                text|               words|         noStopWords|\n",
            "+--------------------+-----+--------------------+--------------------+--------------------+\n",
            "|aba decides again...|    0|aba decides again...|[decides, against...|[decides, communi...|\n",
            "|act fire witnesse...|    1|act fire witnesse...|[fire, witnesses,...|[fire, witnesses,...|\n",
            "|a g calls for inf...|    2|a g calls for inf...|[calls, infrastru...|[calls, infrastru...|\n",
            "|air nz staff in a...|    3|air nz staff in a...|[staff, aust, str...|[staff, aust, str...|\n",
            "|air nz strike to ...|    4|air nz strike to ...|[strike, affect, ...|[strike, affect, ...|\n",
            "|ambitious olsson ...|    5|ambitious olsson ...|[ambitious, olsso...|[ambitious, olsso...|\n",
            "|antic delighted w...|    6|antic delighted w...|[antic, delighted...|[antic, delighted...|\n",
            "|aussie qualifier ...|    7|aussie qualifier ...|[aussie, qualifie...|[aussie, qualifie...|\n",
            "|aust addresses un...|    8|aust addresses un...|[aust, addresses,...|[aust, addresses,...|\n",
            "|australia is lock...|    9|australia is lock...|[australia, locke...|[australia, locke...|\n",
            "|australia to cont...|   10|australia to cont...|[australia, contr...|[australia, contr...|\n",
            "|barca take record...|   11|barca take record...|[barca, take, rec...|[barca, take, rec...|\n",
            "|bathhouse plans m...|   12|bathhouse plans m...|[bathhouse, plans...|[bathhouse, plans...|\n",
            "|big hopes for lau...|   13|big hopes for lau...|[hopes, launcesto...|[hopes, launcesto...|\n",
            "|big plan to boost...|   14|big plan to boost...|[plan, boost, par...|[plan, boost, par...|\n",
            "|blizzard buries u...|   15|blizzard buries u...|[blizzard, buries...|[blizzard, buries...|\n",
            "|brigadier dismiss...|   16|brigadier dismiss...|[brigadier, dismi...|[brigadier, dismi...|\n",
            "|british combat tr...|   17|british combat tr...|[british, combat,...|[british, combat,...|\n",
            "|bryant leads lake...|   18|bryant leads lake...|[bryant, leads, l...|[bryant, leads, l...|\n",
            "|bushfire victims ...|   19|bushfire victims ...|[bushfire, victim...|[bushfire, victim...|\n",
            "+--------------------+-----+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0vVi8o6_aRS"
      },
      "source": [
        "## Lematización"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-I1577Frx5zP"
      },
      "source": [
        "lemma= WordNetLemmatizer()\n",
        "def lematizacion(in_vec):\n",
        "    out_vec = [lemma.lemmatize(w) for w in in_vec]\n",
        "    return out_vec\n",
        "\n",
        "lemma_udf = udf(lambda x:lematizacion(x),ArrayType(StringType()))\n",
        "df_tokens=df_tokens.withColumn('finalwords',lemma_udf(df_tokens['noStopWords']))\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8cmJZejx5zP",
        "outputId": "77322ed5-9349-4a5b-edbb-db993ea057d7"
      },
      "source": [
        "df_tokens.show()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "|           headlines|index|                text|               words|         noStopWords|          finalwords|\n",
            "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "|aba decides again...|    0|aba decides again...|[decides, against...|[decides, communi...|[decides, communi...|\n",
            "|act fire witnesse...|    1|act fire witnesse...|[fire, witnesses,...|[fire, witnesses,...|[fire, witness, m...|\n",
            "|a g calls for inf...|    2|a g calls for inf...|[calls, infrastru...|[calls, infrastru...|[call, infrastruc...|\n",
            "|air nz staff in a...|    3|air nz staff in a...|[staff, aust, str...|[staff, aust, str...|[staff, aust, str...|\n",
            "|air nz strike to ...|    4|air nz strike to ...|[strike, affect, ...|[strike, affect, ...|[strike, affect, ...|\n",
            "|ambitious olsson ...|    5|ambitious olsson ...|[ambitious, olsso...|[ambitious, olsso...|[ambitious, olsso...|\n",
            "|antic delighted w...|    6|antic delighted w...|[antic, delighted...|[antic, delighted...|[antic, delighted...|\n",
            "|aussie qualifier ...|    7|aussie qualifier ...|[aussie, qualifie...|[aussie, qualifie...|[aussie, qualifie...|\n",
            "|aust addresses un...|    8|aust addresses un...|[aust, addresses,...|[aust, addresses,...|[aust, address, s...|\n",
            "|australia is lock...|    9|australia is lock...|[australia, locke...|[australia, locke...|[australia, locke...|\n",
            "|australia to cont...|   10|australia to cont...|[australia, contr...|[australia, contr...|[australia, contr...|\n",
            "|barca take record...|   11|barca take record...|[barca, take, rec...|[barca, take, rec...|[barca, take, rec...|\n",
            "|bathhouse plans m...|   12|bathhouse plans m...|[bathhouse, plans...|[bathhouse, plans...|[bathhouse, plan,...|\n",
            "|big hopes for lau...|   13|big hopes for lau...|[hopes, launcesto...|[hopes, launcesto...|[hope, launceston...|\n",
            "|big plan to boost...|   14|big plan to boost...|[plan, boost, par...|[plan, boost, par...|[plan, boost, par...|\n",
            "|blizzard buries u...|   15|blizzard buries u...|[blizzard, buries...|[blizzard, buries...|[blizzard, buries...|\n",
            "|brigadier dismiss...|   16|brigadier dismiss...|[brigadier, dismi...|[brigadier, dismi...|[brigadier, dismi...|\n",
            "|british combat tr...|   17|british combat tr...|[british, combat,...|[british, combat,...|[british, combat,...|\n",
            "|bryant leads lake...|   18|bryant leads lake...|[bryant, leads, l...|[bryant, leads, l...|[bryant, lead, la...|\n",
            "|bushfire victims ...|   19|bushfire victims ...|[bushfire, victim...|[bushfire, victim...|[bushfire, victim...|\n",
            "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qkFYR-z058A",
        "scrolled": false,
        "outputId": "6dfecf67-8a3d-41f2-f238-8ef7a90aee8a"
      },
      "source": [
        "df_tokens.printSchema()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- headlines: string (nullable = true)\n",
            " |-- index: long (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            " |-- words: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- noStopWords: string (nullable = true)\n",
            " |-- finalwords: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pUNuGY5x5zQ"
      },
      "source": [
        "###N-Grams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Cx8uqYg058B"
      },
      "source": [
        "## Vectorizar con unigramas y bigramas\n",
        "\n",
        "En pyspark tenemos que primero que crear todos los n-gramas que nos interesen y luego utilizar el modelo CountVectorizer y unirlo todo utilizando VectorAssembler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bfcand3cx5zR"
      },
      "source": [
        "def build_ngrams(inputCol=\"finalwords\", n=2):\n",
        "\n",
        "    ngrams = [\n",
        "        NGram(n=i, inputCol=\"finalwords\", outputCol=\"{0}_grams\".format(i))\n",
        "        for i in range(1, n + 1)\n",
        "    ]\n",
        "\n",
        "    vectorizers = [\n",
        "        CountVectorizer(inputCol=\"{0}_grams\".format(i),\n",
        "            outputCol=\"{0}_counts\".format(i), minDF=20)\n",
        "        for i in range(1, n + 1)\n",
        "    ]\n",
        "\n",
        "    assembler = [VectorAssembler(\n",
        "        inputCols=[\"{0}_counts\".format(i) for i in range(1, n + 1)],\n",
        "        outputCol=\"features_cv\"\n",
        "    )]\n",
        "\n",
        "    return Pipeline(stages=ngrams + vectorizers + assembler)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_-UPt-fx5zS",
        "outputId": "1df573a0-bd42-4f64-832b-8bb3393dec56"
      },
      "source": [
        "%%time\n",
        "# TF\n",
        "pipline_vectorizer_ngrams = build_ngrams().fit(df_tokens)\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 577 ms, sys: 70.7 ms, total: 648 ms\n",
            "Wall time: 2min 29s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwMkSZewx5zS",
        "outputId": "d09cd65d-812d-4eee-959d-3335aaf194b9"
      },
      "source": [
        "pipline_vectorizer_ngrams.stages"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[NGram_425ddb78a44e,\n",
              " NGram_a4e1798d38dd,\n",
              " CountVectorizerModel: uid=CountVectorizer_2843d8e3b008, vocabularySize=16599,\n",
              " CountVectorizerModel: uid=CountVectorizer_cd1a46cd2af6, vocabularySize=19985,\n",
              " VectorAssembler_75e2e8977f81]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_A5mT-Nex5zT",
        "outputId": "f87498eb-3230-48f8-baee-689e4d18141c"
      },
      "source": [
        "vectorizers = [s for s in pipline_vectorizer_ngrams.stages if isinstance(s, CountVectorizerModel)]\n",
        "vocabArray = [v.vocabulary for v in vectorizers]\n",
        "len(vocabArray)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCR-Sk2Ox5zU"
      },
      "source": [
        "# Con cuantas palabras y bigramas nos quedamos\n",
        "palabras=vocabArray[0]\n",
        "biGramas=vocabArray[1]"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uB5gcNQpx5zU",
        "outputId": "2ec120bb-cddb-430d-811c-f081bf69f566"
      },
      "source": [
        "# muestra de bigramas\n",
        "biGramas[0:20]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['gold coast',\n",
              " 'country hour',\n",
              " 'donald trump',\n",
              " 'face court',\n",
              " 'pleads guilty',\n",
              " 'asylum seeker',\n",
              " 'mental health',\n",
              " 'climate change',\n",
              " 'police investigate',\n",
              " 'north korea',\n",
              " 'police probe',\n",
              " 'broken hill',\n",
              " 'share market',\n",
              " 'rate rise',\n",
              " 'royal commission',\n",
              " 'police officer',\n",
              " 'plane crash',\n",
              " 'body found',\n",
              " 'front court',\n",
              " 'govt urged']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPxdkeGyx5zV",
        "outputId": "e1bf697e-58f9-4d2b-d6b6-97e9e07e3bbf"
      },
      "source": [
        "vocabulario = palabras + biGramas\n",
        "len(vocabulario)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36584"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlmwPcQRx5zV"
      },
      "source": [
        "__ahora hacemos la transformación con la vectorización hecha para obtener TF__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWEIU0Jvx5zW",
        "outputId": "c4a24d15-f138-4288-ee53-6ebf5b45befb"
      },
      "source": [
        "%%time\n",
        "result_cv = pipline_vectorizer_ngrams.transform(df_tokens)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 20.4 ms, sys: 1.5 ms, total: 21.9 ms\n",
            "Wall time: 803 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YakK2smIx5zW",
        "outputId": "1d8e52ee-aadd-4e79-f991-d6d22f3d6216"
      },
      "source": [
        "result_cv.columns  # la vectorización se encuentra en la última columna"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['headlines',\n",
              " 'index',\n",
              " 'text',\n",
              " 'words',\n",
              " 'noStopWords',\n",
              " 'finalwords',\n",
              " '1_grams',\n",
              " '2_grams',\n",
              " '1_counts',\n",
              " '2_counts',\n",
              " 'features_cv']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAatJjGVx5zX",
        "outputId": "fee1a4d8-0246-4fcb-8eeb-a37c640833de"
      },
      "source": [
        "result_cv.take(1)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(headlines='aba decides against community broadcasting licence', index=0, text='aba decides against community broadcasting licence', words=['decides', 'against', 'community', 'broadcasting', 'licence'], noStopWords='[decides, community, broadcasting, licence]', finalwords=['decides', 'community', 'broadcasting', 'licence'], 1_grams=['decides', 'community', 'broadcasting', 'licence'], 2_grams=['decides community', 'community broadcasting', 'broadcasting licence'], 1_counts=SparseVector(16599, {111: 1.0, 958: 1.0, 5213: 1.0, 8099: 1.0}), 2_counts=SparseVector(19985, {}), features_cv=SparseVector(36584, {111: 1.0, 958: 1.0, 5213: 1.0, 8099: 1.0}))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNJ16027x5zX"
      },
      "source": [
        "__es el turno de obtener IDF__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekCwipqQ058C",
        "outputId": "fa77e7ce-a371-4ade-c912-581972ddb28b"
      },
      "source": [
        "%%time\n",
        "# IDF\n",
        "idf = IDF(inputCol=\"features_cv\", outputCol=\"features\")\n",
        "idfModel = idf.fit(result_cv)\n",
        "result_tfidf = idfModel.transform(result_cv) "
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 286 ms, sys: 38.7 ms, total: 325 ms\n",
            "Wall time: 1min 17s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FC-6u-a4x5zY",
        "outputId": "b236416a-2139-440d-8ce6-f4e5f7a9fd65"
      },
      "source": [
        "result_tfidf.take(1)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(headlines='aba decides against community broadcasting licence', index=0, text='aba decides against community broadcasting licence', words=['decides', 'against', 'community', 'broadcasting', 'licence'], noStopWords='[decides, community, broadcasting, licence]', finalwords=['decides', 'community', 'broadcasting', 'licence'], 1_grams=['decides', 'community', 'broadcasting', 'licence'], 2_grams=['decides community', 'community broadcasting', 'broadcasting licence'], 1_counts=SparseVector(16599, {111: 1.0, 958: 1.0, 5213: 1.0, 8099: 1.0}), 2_counts=SparseVector(19985, {}), features_cv=SparseVector(36584, {111: 1.0, 958: 1.0, 5213: 1.0, 8099: 1.0}), features=SparseVector(36584, {111: 5.2528, 958: 6.8735, 5213: 9.0088, 8099: 9.729}))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emHv8FE8058C"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdIW1IaN058C",
        "scrolled": true,
        "outputId": "6bfbad47-bfae-4985-c28e-0f83f0f0e939"
      },
      "source": [
        "%%time\n",
        "num_topics=30\n",
        "max_iterations=50\n",
        "lda = LDA(k=num_topics, maxIter=max_iterations)\n",
        "ldaModel = lda.fit(result_tfidf)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2.18 s, sys: 235 ms, total: 2.42 s\n",
            "Wall time: 10min 28s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KeWpKkiD08f"
      },
      "source": [
        "### Guardar los modelos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ws8YqHuUO3wJ"
      },
      "source": [
        "print(ldaModel.isDistributed())\n",
        "path = \"/content/drive/MyDrive/Modelos/modelosLDA/\"\n",
        "\n",
        "model_number = '2'\n",
        "pipline_vectorizer_ngrams.save(path + 'PipelineVectorizerModel'+ model_number)    # Modelo BOW\n",
        "ldaModel.save(path + 'LDAModel'+ model_number)  # Modelo entrenado\n",
        "lda.save(path + 'LDA_'+ model_number)\n",
        "idfModel.save(path + 'idfModel'+ model_number) "
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6JmCQy7EVpX"
      },
      "source": [
        "# Cargar modelos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKao5qT3Efyd"
      },
      "source": [
        "path = \"/content/drive/MyDrive/Modelos/modelosLDA/\"\n",
        "model_number = '2'\n",
        "pipline_vectorizer_ngrams = PipelineModel.load(path + 'PipelineVectorizerModel'+ model_number )   # Modelo BOW\n",
        "#lda = LocalLDAModel.load(path + 'LDA_'+ model_number)\n",
        "ldaModel = LocalLDAModel.load(path + 'LDAModel'+ model_number)               # Modelo entrenado"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7xQOgTQ058D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c39034b9-a9d2-4f0b-becc-7397518d57dc"
      },
      "source": [
        "# Print topics and top-weighted terms\n",
        "numTopics = 30  # cantidad de topicos a explorar\n",
        "topics = ldaModel.describeTopics(maxTermsPerTopic=5)\n",
        "ListOfIndexToWords = udf(lambda wl: list([vocabulario[w] for w in wl]))\n",
        "FormatNumbers = udf(lambda nl: [\"{:1.4f}\".format(x) for x in nl])\n",
        "\n",
        "toptopics = topics.select((topics.topic + 1).alias('topic'),\n",
        "                          ListOfIndexToWords(topics.termIndices).alias('words'),\n",
        "                          FormatNumbers(topics.termWeights).alias('weights'))\n",
        "toptopics.show(truncate=False, n=numTopics)\n",
        "print('Topics:', numTopics, 'Vocabulary:', len(vocabArray))\n",
        "\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+------------------------------------------------+----------------------------------------+\n",
            "|topic|words                                           |weights                                 |\n",
            "+-----+------------------------------------------------+----------------------------------------+\n",
            "|1    |[second, flood, boost, start, england]          |[0.0118, 0.0113, 0.0111, 0.0106, 0.0096]|\n",
            "|2    |[claim, dead, three, shot, killed]              |[0.0203, 0.0159, 0.0111, 0.0094, 0.0084]|\n",
            "|3    |[child, probe, fire, police, abuse]             |[0.0235, 0.0170, 0.0138, 0.0113, 0.0104]|\n",
            "|4    |[price, close, jailed, officer, china]          |[0.0171, 0.0161, 0.0159, 0.0133, 0.0125]|\n",
            "|5    |[council, land, iraq, drug, troop]              |[0.0185, 0.0132, 0.0118, 0.0088, 0.0086]|\n",
            "|6    |[year, west, road, storm, blue]                 |[0.0134, 0.0108, 0.0101, 0.0096, 0.0096]|\n",
            "|7    |[market, local, body, aussie, medium]           |[0.0191, 0.0126, 0.0121, 0.0118, 0.0116]|\n",
            "|8    |[action, legal, injury, considers, public]      |[0.0153, 0.0102, 0.0102, 0.0099, 0.0097]|\n",
            "|9    |[deal, future, north, korea, program]           |[0.0256, 0.0186, 0.0160, 0.0139, 0.0112]|\n",
            "|10   |[charged, court, told, canberra, appeal]        |[0.0169, 0.0129, 0.0116, 0.0115, 0.0114]|\n",
            "|11   |[tasmania, force, rail, target, record]         |[0.0143, 0.0125, 0.0117, 0.0102, 0.0102]|\n",
            "|12   |[show, could, warns, drought, bill]             |[0.0282, 0.0197, 0.0196, 0.0176, 0.0136]|\n",
            "|13   |[health, worker, law, week, turn]               |[0.0201, 0.0172, 0.0114, 0.0095, 0.0085]|\n",
            "|14   |[community, seek, cut, abbott, player]          |[0.0139, 0.0125, 0.0106, 0.0101, 0.0099]|\n",
            "|15   |[guilty, threat, weather, break, push]          |[0.0112, 0.0104, 0.0101, 0.0101, 0.0094]|\n",
            "|16   |[business, labor, speaks, violence, hold]       |[0.0221, 0.0166, 0.0155, 0.0154, 0.0142]|\n",
            "|17   |[school, kill, hit, right, street]              |[0.0218, 0.0196, 0.0163, 0.0128, 0.0126]|\n",
            "|18   |[move, ahead, change, final, climate]           |[0.0146, 0.0124, 0.0122, 0.0119, 0.0105]|\n",
            "|19   |[crash, dy, green, leader, service]             |[0.0260, 0.0257, 0.0196, 0.0179, 0.0146]|\n",
            "|20   |[election, trump, offer, donald, food]          |[0.0254, 0.0211, 0.0187, 0.0129, 0.0129]|\n",
            "|21   |[murder, accused, face, charge, country]        |[0.0209, 0.0202, 0.0198, 0.0191, 0.0173]|\n",
            "|22   |[black, asylum, investigation, royal, interview]|[0.0097, 0.0090, 0.0086, 0.0085, 0.0084]|\n",
            "|23   |[star, share, shark, time, training]            |[0.0138, 0.0118, 0.0109, 0.0100, 0.0090]|\n",
            "|24   |[student, farmer, bail, arrested, condition]    |[0.0175, 0.0127, 0.0120, 0.0099, 0.0086]|\n",
            "|25   |[death, rise, million, island, toll]            |[0.0165, 0.0151, 0.0119, 0.0114, 0.0112]|\n",
            "|26   |[water, job, welcome, rate, vote]               |[0.0197, 0.0186, 0.0180, 0.0166, 0.0146]|\n",
            "|27   |[coast, gold, inquiry, coronavirus, gold coast] |[0.0173, 0.0172, 0.0132, 0.0120, 0.0106]|\n",
            "|28   |[driver, defends, company, sport, march]        |[0.0180, 0.0119, 0.0117, 0.0111, 0.0091]|\n",
            "|29   |[rural, national, centre, mine, doctor]         |[0.0236, 0.0203, 0.0195, 0.0187, 0.0153]|\n",
            "|30   |[assault, four, debate, tourist, control]       |[0.0161, 0.0152, 0.0104, 0.0099, 0.0099]|\n",
            "+-----+------------------------------------------------+----------------------------------------+\n",
            "\n",
            "Topics: 30 Vocabulary: 2\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}