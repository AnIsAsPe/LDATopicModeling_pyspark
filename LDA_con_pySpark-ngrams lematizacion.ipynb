{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hi6OGDnO2f2"
   },
   "source": [
    "# LDA con pySpark \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7851\r\n"
     ]
    }
   ],
   "source": [
    "!echo $(($(getconf _PHYS_PAGES) * $(getconf PAGE_SIZE) / (1024 * 1024)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5wmR_zBxhrD"
   },
   "source": [
    "## Crear Sesión Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9-Mq8KI6xjd-"
   },
   "outputs": [],
   "source": [
    "# Crear una sesión de spark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"Linux\")\\\n",
    "        .config('spark.ui.port', '4050')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "id": "0-c_EzRxxxjN",
    "outputId": "7fb0a398-12c5-4081-d6b8-1c02815e3503"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.10:4051\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Linux</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f3b68839eb0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0TxAujHu236V"
   },
   "source": [
    "## Importar bibliotecas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AhBpsCzr296y",
    "outputId": "0f60e3c6-d5d1-457a-9cd7-21b24a5c4c70"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/anaisabel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/anaisabel/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf, concat, split, col\n",
    "from pyspark.ml.feature import RegexTokenizer, NGram, VectorAssembler, CountVectorizer, IDF\n",
    "from pyspark.ml.clustering import LDA, LocalLDAModel\n",
    "from pyspark.ml import Pipeline\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')  #WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "koJQrSJL095M"
   },
   "source": [
    "# Leer los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x2liL4jWzMgc",
    "outputId": "d47d11e2-3aac-41b2-cd21-f16150b293db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de renglones:  1226258\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"datos/abcnews-date-text.csv\",header=True)\n",
    "\n",
    "print('Cantidad de renglones: ', df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eeCfIBYb3j4k",
    "outputId": "925578df-ae0f-4828-aeb7-e795e259011b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- publish_date: string (nullable = true)\n",
      " |-- headline_text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema() # explorar la estructura el dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pqEhSGci5HzE",
    "outputId": "05248129-a407-4176-adb4-be61b5ceb637"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(publish_date='20030219', headline_text='aba decides against community broadcasting licence'),\n",
       " Row(publish_date='20030219', headline_text='act fire witnesses must be aware of defamation'),\n",
       " Row(publish_date='20030219', headline_text='a g calls for infrastructure protection summit')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tdpu1WZc5Tbk",
    "outputId": "fb3a3bca-2573-4270-dc32-8c529a47597c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|publish_date|       headline_text|\n",
      "+------------+--------------------+\n",
      "|    20030219|aba decides again...|\n",
      "|    20030219|act fire witnesse...|\n",
      "|    20030219|a g calls for inf...|\n",
      "|    20030219|air nz staff in a...|\n",
      "|    20030219|air nz strike to ...|\n",
      "|    20030219|ambitious olsson ...|\n",
      "|    20030219|antic delighted w...|\n",
      "|    20030219|aussie qualifier ...|\n",
      "|    20030219|aust addresses un...|\n",
      "|    20030219|australia is lock...|\n",
      "+------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10) # default 20 renglones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "araSDXeh0gKM",
    "outputId": "69b1437e-dcc7-48e0-faa6-fb9e1c249b75"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(headline_text='aba decides against community broadcasting licence')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('headline_text').take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "sjr90qIi0M-L",
    "outputId": "eaf30c78-ba15-4365-c135-48bfa4678127"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aba decides against community broadcasting licence'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# raw text of the first entry \n",
    "df.select('headline_text').head(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0aUQo7g71dsE",
    "outputId": "62a43940-140a-407b-801d-48b2c8953bce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2RCzNuN0579"
   },
   "source": [
    "Queremos un inice consecutivo, para ello vamos a utilizar solo la columna 'hedline_text' y usando rdd creamos el indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GIkeM7R51RHi",
    "outputId": "ebd54841-8f22-49c5-cb4e-9c9c42546840"
   },
   "outputs": [],
   "source": [
    "texts = df.rdd.map(lambda x: x['headline_text'])\n",
    "headlines=texts.zipWithIndex( )                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Um4zlKJa057-"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(spark)\n",
    "#Creating dataframe\n",
    "data = sqlContext.createDataFrame(headlines, [\"headlines\",'index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYyebrRS2hdQ"
   },
   "source": [
    "# Preprocesar Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVEOl5cJ057-"
   },
   "source": [
    "## Normalizar y tokenizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "LTrWBzku057-"
   },
   "outputs": [],
   "source": [
    "removePunct = udf(lambda s: s.strip().lower(), StringType())\n",
    "\n",
    "data_norm = data.withColumn(\"text\", removePunct(data.headlines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "L0R3njG0057_"
   },
   "outputs": [],
   "source": [
    "# tokenize \n",
    "tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\",\n",
    "                           gaps=True, pattern=r'\\s+', minTokenLength=4)\n",
    "df_tokens = tokenizer.transform(data_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7CClFRuE057_",
    "outputId": "3dd98b66-bb7c-463f-f9e0-5b82931dac0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+\n",
      "|           headlines|index|                text|               words|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "|aba decides again...|    0|aba decides again...|[decides, against...|\n",
      "|act fire witnesse...|    1|act fire witnesse...|[fire, witnesses,...|\n",
      "|a g calls for inf...|    2|a g calls for inf...|[calls, infrastru...|\n",
      "|air nz staff in a...|    3|air nz staff in a...|[staff, aust, str...|\n",
      "|air nz strike to ...|    4|air nz strike to ...|[strike, affect, ...|\n",
      "|ambitious olsson ...|    5|ambitious olsson ...|[ambitious, olsso...|\n",
      "|antic delighted w...|    6|antic delighted w...|[antic, delighted...|\n",
      "|aussie qualifier ...|    7|aussie qualifier ...|[aussie, qualifie...|\n",
      "|aust addresses un...|    8|aust addresses un...|[aust, addresses,...|\n",
      "|australia is lock...|    9|australia is lock...|[australia, locke...|\n",
      "|australia to cont...|   10|australia to cont...|[australia, contr...|\n",
      "|barca take record...|   11|barca take record...|[barca, take, rec...|\n",
      "|bathhouse plans m...|   12|bathhouse plans m...|[bathhouse, plans...|\n",
      "|big hopes for lau...|   13|big hopes for lau...|[hopes, launcesto...|\n",
      "|big plan to boost...|   14|big plan to boost...|[plan, boost, par...|\n",
      "|blizzard buries u...|   15|blizzard buries u...|[blizzard, buries...|\n",
      "|brigadier dismiss...|   16|brigadier dismiss...|[brigadier, dismi...|\n",
      "|british combat tr...|   17|british combat tr...|[british, combat,...|\n",
      "|bryant leads lake...|   18|bryant leads lake...|[bryant, leads, l...|\n",
      "|bushfire victims ...|   19|bushfire victims ...|[bushfire, victim...|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tokens.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1NMSndtR057_"
   },
   "source": [
    "## Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "eKKKK5F1058A"
   },
   "outputs": [],
   "source": [
    "stopwords = stopwords.words(\"english\")\n",
    "removeStop=udf(lambda word: [x for x in word if x not in stopwords])\n",
    "df_tokens=df_tokens.withColumn('noStopWords',removeStop(df_tokens['words']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yCqO4X-2058A",
    "outputId": "a4a0146d-2c0b-4ff2-87d1-eb5fc5c79950"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|           headlines|index|                text|               words|         noStopWords|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|aba decides again...|    0|aba decides again...|[decides, against...|[decides, communi...|\n",
      "|act fire witnesse...|    1|act fire witnesse...|[fire, witnesses,...|[fire, witnesses,...|\n",
      "|a g calls for inf...|    2|a g calls for inf...|[calls, infrastru...|[calls, infrastru...|\n",
      "|air nz staff in a...|    3|air nz staff in a...|[staff, aust, str...|[staff, aust, str...|\n",
      "|air nz strike to ...|    4|air nz strike to ...|[strike, affect, ...|[strike, affect, ...|\n",
      "|ambitious olsson ...|    5|ambitious olsson ...|[ambitious, olsso...|[ambitious, olsso...|\n",
      "|antic delighted w...|    6|antic delighted w...|[antic, delighted...|[antic, delighted...|\n",
      "|aussie qualifier ...|    7|aussie qualifier ...|[aussie, qualifie...|[aussie, qualifie...|\n",
      "|aust addresses un...|    8|aust addresses un...|[aust, addresses,...|[aust, addresses,...|\n",
      "|australia is lock...|    9|australia is lock...|[australia, locke...|[australia, locke...|\n",
      "|australia to cont...|   10|australia to cont...|[australia, contr...|[australia, contr...|\n",
      "|barca take record...|   11|barca take record...|[barca, take, rec...|[barca, take, rec...|\n",
      "|bathhouse plans m...|   12|bathhouse plans m...|[bathhouse, plans...|[bathhouse, plans...|\n",
      "|big hopes for lau...|   13|big hopes for lau...|[hopes, launcesto...|[hopes, launcesto...|\n",
      "|big plan to boost...|   14|big plan to boost...|[plan, boost, par...|[plan, boost, par...|\n",
      "|blizzard buries u...|   15|blizzard buries u...|[blizzard, buries...|[blizzard, buries...|\n",
      "|brigadier dismiss...|   16|brigadier dismiss...|[brigadier, dismi...|[brigadier, dismi...|\n",
      "|british combat tr...|   17|british combat tr...|[british, combat,...|[british, combat,...|\n",
      "|bryant leads lake...|   18|bryant leads lake...|[bryant, leads, l...|[bryant, leads, l...|\n",
      "|bushfire victims ...|   19|bushfire victims ...|[bushfire, victim...|[bushfire, victim...|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tokens.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0vVi8o6_aRS"
   },
   "source": [
    "## Lematización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma= WordNetLemmatizer()\n",
    "def lematizacion(in_vec):\n",
    "    out_vec = [lemma.lemmatize(w) for w in in_vec]\n",
    "    return out_vec\n",
    "\n",
    "lemma_udf = udf(lambda x:lematizacion(x),ArrayType(StringType()))\n",
    "df_tokens=df_tokens.withColumn('finalwords',lemma_udf(df_tokens['noStopWords']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|           headlines|index|                text|               words|         noStopWords|          finalwords|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|aba decides again...|    0|aba decides again...|[decides, against...|[decides, communi...|[decides, communi...|\n",
      "|act fire witnesse...|    1|act fire witnesse...|[fire, witnesses,...|[fire, witnesses,...|[fire, witness, m...|\n",
      "|a g calls for inf...|    2|a g calls for inf...|[calls, infrastru...|[calls, infrastru...|[call, infrastruc...|\n",
      "|air nz staff in a...|    3|air nz staff in a...|[staff, aust, str...|[staff, aust, str...|[staff, aust, str...|\n",
      "|air nz strike to ...|    4|air nz strike to ...|[strike, affect, ...|[strike, affect, ...|[strike, affect, ...|\n",
      "|ambitious olsson ...|    5|ambitious olsson ...|[ambitious, olsso...|[ambitious, olsso...|[ambitious, olsso...|\n",
      "|antic delighted w...|    6|antic delighted w...|[antic, delighted...|[antic, delighted...|[antic, delighted...|\n",
      "|aussie qualifier ...|    7|aussie qualifier ...|[aussie, qualifie...|[aussie, qualifie...|[aussie, qualifie...|\n",
      "|aust addresses un...|    8|aust addresses un...|[aust, addresses,...|[aust, addresses,...|[aust, address, s...|\n",
      "|australia is lock...|    9|australia is lock...|[australia, locke...|[australia, locke...|[australia, locke...|\n",
      "|australia to cont...|   10|australia to cont...|[australia, contr...|[australia, contr...|[australia, contr...|\n",
      "|barca take record...|   11|barca take record...|[barca, take, rec...|[barca, take, rec...|[barca, take, rec...|\n",
      "|bathhouse plans m...|   12|bathhouse plans m...|[bathhouse, plans...|[bathhouse, plans...|[bathhouse, plan,...|\n",
      "|big hopes for lau...|   13|big hopes for lau...|[hopes, launcesto...|[hopes, launcesto...|[hope, launceston...|\n",
      "|big plan to boost...|   14|big plan to boost...|[plan, boost, par...|[plan, boost, par...|[plan, boost, par...|\n",
      "|blizzard buries u...|   15|blizzard buries u...|[blizzard, buries...|[blizzard, buries...|[blizzard, buries...|\n",
      "|brigadier dismiss...|   16|brigadier dismiss...|[brigadier, dismi...|[brigadier, dismi...|[brigadier, dismi...|\n",
      "|british combat tr...|   17|british combat tr...|[british, combat,...|[british, combat,...|[british, combat,...|\n",
      "|bryant leads lake...|   18|bryant leads lake...|[bryant, leads, l...|[bryant, leads, l...|[bryant, lead, la...|\n",
      "|bushfire victims ...|   19|bushfire victims ...|[bushfire, victim...|[bushfire, victim...|[bushfire, victim...|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tokens.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5qkFYR-z058A",
    "outputId": "f4070036-0e2f-42cd-99de-be334de6c7b6",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- headlines: string (nullable = true)\n",
      " |-- index: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- noStopWords: string (nullable = true)\n",
      " |-- finalwords: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tokens.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Cx8uqYg058B"
   },
   "source": [
    "## Vectorizar con unigramas y bigramas\n",
    "\n",
    "En pyspark tenemos que primero que crear todos los n-gramas que nos interesen y luego utilizar el modelo CountVectorizer y unirlo todo utilizando VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ngrams(inputCol=\"finalwords\", n=2):\n",
    "\n",
    "    ngrams = [\n",
    "        NGram(n=i, inputCol=\"finalwords\", outputCol=\"{0}_grams\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "\n",
    "    vectorizers = [\n",
    "        CountVectorizer(inputCol=\"{0}_grams\".format(i),\n",
    "            outputCol=\"{0}_counts\".format(i), minDF=20)\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "\n",
    "    assembler = [VectorAssembler(\n",
    "        inputCols=[\"{0}_counts\".format(i) for i in range(1, n + 1)],\n",
    "        outputCol=\"features_cv\"\n",
    "    )]\n",
    "\n",
    "    return Pipeline(stages=ngrams + vectorizers + assembler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.4 ms, sys: 7.93 ms, total: 56.3 ms\n",
      "Wall time: 2min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# TF\n",
    "pipline_vectorizer_ngrams = build_ngrams().fit(df_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NGram_4bd4ad453bcd,\n",
       " NGram_bcbfc2431b84,\n",
       " CountVectorizerModel: uid=CountVectorizer_dcb2c6c639e9, vocabularySize=16599,\n",
       " CountVectorizerModel: uid=CountVectorizer_cd8ca00483ce, vocabularySize=19985,\n",
       " VectorAssembler_2423b77deccf]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipline_vectorizer_ngrams.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizers = [s for s in pipline_vectorizer_ngrams.stages if isinstance(s, CountVectorizerModel)]\n",
    "vocabArray = [v.vocabulary for v in vectorizers]\n",
    "len(vocabArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con cuantas palabras y bigramas nos quedamos\n",
    "palabras=vocabArray[0]\n",
    "biGramas=vocabArray[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gold coast',\n",
       " 'country hour',\n",
       " 'donald trump',\n",
       " 'face court',\n",
       " 'pleads guilty',\n",
       " 'asylum seeker',\n",
       " 'mental health',\n",
       " 'climate change',\n",
       " 'police investigate',\n",
       " 'north korea',\n",
       " 'police probe',\n",
       " 'broken hill',\n",
       " 'share market',\n",
       " 'rate rise',\n",
       " 'royal commission',\n",
       " 'police officer',\n",
       " 'plane crash',\n",
       " 'body found',\n",
       " 'front court',\n",
       " 'govt urged']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# muestra de bigramas\n",
    "biGramas[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36584"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulario = palabras + biGramas\n",
    "len(vocabulario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ahora hacemos la transformación con la vectorización hecha para obtener TF__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.8 ms, sys: 313 µs, total: 31.1 ms\n",
      "Wall time: 585 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result_cv = pipline_vectorizer_ngrams.transform(df_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['headlines',\n",
       " 'index',\n",
       " 'text',\n",
       " 'words',\n",
       " 'noStopWords',\n",
       " 'finalwords',\n",
       " '1_grams',\n",
       " '2_grams',\n",
       " '1_counts',\n",
       " '2_counts',\n",
       " 'features_cv']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_cv.columns  # la vectorización se encuentra en la última columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(headlines='aba decides against community broadcasting licence', index=0, text='aba decides against community broadcasting licence', words=['decides', 'against', 'community', 'broadcasting', 'licence'], noStopWords='[decides, community, broadcasting, licence]', finalwords=['decides', 'community', 'broadcasting', 'licence'], 1_grams=['decides', 'community', 'broadcasting', 'licence'], 2_grams=['decides community', 'community broadcasting', 'broadcasting licence'], 1_counts=SparseVector(16599, {111: 1.0, 958: 1.0, 5213: 1.0, 8099: 1.0}), 2_counts=SparseVector(19985, {}), features_cv=SparseVector(36584, {111: 1.0, 958: 1.0, 5213: 1.0, 8099: 1.0}))]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_cv.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__es el turno de obtener IDF__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ekCwipqQ058C",
    "outputId": "ed3fbc6b-53aa-454d-9b90-1e9f8fb6c257"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14 ms, sys: 0 ns, total: 14 ms\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# IDF\n",
    "idf = IDF(inputCol=\"features_cv\", outputCol=\"features\")\n",
    "idfModel = idf.fit(result_cv)\n",
    "result_tfidf = idfModel.transform(result_cv) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(headlines='aba decides against community broadcasting licence', index=0, text='aba decides against community broadcasting licence', words=['decides', 'against', 'community', 'broadcasting', 'licence'], noStopWords='[decides, community, broadcasting, licence]', finalwords=['decides', 'community', 'broadcasting', 'licence'], 1_grams=['decides', 'community', 'broadcasting', 'licence'], 2_grams=['decides community', 'community broadcasting', 'broadcasting licence'], 1_counts=SparseVector(16599, {111: 1.0, 958: 1.0, 5213: 1.0, 8099: 1.0}), 2_counts=SparseVector(19985, {}), features_cv=SparseVector(36584, {111: 1.0, 958: 1.0, 5213: 1.0, 8099: 1.0}), features=SparseVector(36584, {111: 5.2528, 958: 6.8735, 5213: 9.0088, 8099: 9.729}))]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_tfidf.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emHv8FE8058C"
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AjosOSGO91DC",
    "outputId": "f5a19a97-5e3c-4f6e-aa9d-f416aec3e0f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing documents:  981475 244783\n",
      "CPU times: user 16 ms, sys: 1.54 ms, total: 17.6 ms\n",
      "Wall time: 3min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#split 80% train set and 20% test set\n",
    "df_training, df_testing = result_tfidf.randomSplit([0.8, 0.2], 1)\n",
    "print('Training and testing documents: ', df_training.count(), df_testing.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gdIW1IaN058C",
    "outputId": "675c3948-24a9-42b1-d34f-af24e09114a6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.4 ms, sys: 24.9 ms, total: 74.2 ms\n",
      "Wall time: 12min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_topics=30\n",
    "max_iterations=50\n",
    "lda = LDA(k=num_topics, maxIter=max_iterations)\n",
    "ldaModel = lda.fit(result_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KeWpKkiD08f"
   },
   "source": [
    "### Guardar los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V2XGGi_1qKVj",
    "outputId": "368bf5e5-70d9-4f0f-afa4-e4ab6594dc5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(ldaModel.isDistributed())\n",
    "path = \"modelos/\"\n",
    "\n",
    "model_number = '2'\n",
    "pipline_vectorizer_ngrams.save(path + 'PipelineVectorizerModel'+ model_number)    # Modelo BOW\n",
    "ldaModel.save(path + 'LDAModel'+ model_number)  # Modelo entrenado\n",
    "lda.save(path + 'LDA_'+ model_number)\n",
    "idfModel.save(path + 'idfModel'+ model_number) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6JmCQy7EVpX"
   },
   "source": [
    "# Cargar modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "uKao5qT3Efyd"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.clustering import LocalLDAModel\n",
    "path = \"modelos/\"\n",
    "model_number = '2'\n",
    "pipline_vectorizer_ngrams = PipelineModel.load(path + 'PipelineVectorizerModel'+ model_number )   # Modelo BOW\n",
    "#lda = LocalLDAModel.load(path + 'LDA_'+ model_number)\n",
    "ldaModel = LocalLDAModel.load(path + 'LDAModel'+ model_number)               # Modelo entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "G7xQOgTQ058D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------------------------------+----------------------------------------+\n",
      "|topic|words                                       |weights                                 |\n",
      "+-----+--------------------------------------------+----------------------------------------+\n",
      "|1    |[face, court, budget, challenge, delay]     |[0.0219, 0.0164, 0.0159, 0.0111, 0.0108]|\n",
      "|2    |[attack, charged, teen, hobart, assault]    |[0.0362, 0.0266, 0.0226, 0.0145, 0.0143]|\n",
      "|3    |[gold, trump, coast, offer, premier]        |[0.0169, 0.0145, 0.0133, 0.0123, 0.0109]|\n",
      "|4    |[island, station, title, number, highway]   |[0.0157, 0.0138, 0.0131, 0.0117, 0.0109]|\n",
      "|5    |[murder, business, spark, storm, accused]   |[0.0193, 0.0121, 0.0105, 0.0096, 0.0086]|\n",
      "|6    |[high, arrested, rule, threat, told]        |[0.0155, 0.0147, 0.0116, 0.0116, 0.0114]|\n",
      "|7    |[violence, second, india, aust, strike]     |[0.0115, 0.0113, 0.0108, 0.0100, 0.0093]|\n",
      "|8    |[election, club, scott, alice, video]       |[0.0229, 0.0193, 0.0159, 0.0152, 0.0135]|\n",
      "|9    |[guilty, win, injured, battle, pleads]      |[0.0172, 0.0130, 0.0115, 0.0109, 0.0097]|\n",
      "|10   |[rural, news, national, price, world]       |[0.0220, 0.0170, 0.0162, 0.0160, 0.0157]|\n",
      "|11   |[share, safety, market, final, liberal]     |[0.0136, 0.0118, 0.0114, 0.0109, 0.0105]|\n",
      "|12   |[dy, farm, bushfire, wind, toll]            |[0.0203, 0.0145, 0.0103, 0.0098, 0.0095]|\n",
      "|13   |[iraq, look, find, speaks, hold]            |[0.0134, 0.0104, 0.0099, 0.0091, 0.0085]|\n",
      "|14   |[officer, ahead, housing, clash, launch]    |[0.0151, 0.0134, 0.0133, 0.0129, 0.0115]|\n",
      "|15   |[life, people, return, street, covid]       |[0.0187, 0.0152, 0.0141, 0.0125, 0.0123]|\n",
      "|16   |[school, opposition, student, cut, teacher] |[0.0168, 0.0155, 0.0135, 0.0131, 0.0114]|\n",
      "|17   |[worker, push, head, free, jailed]          |[0.0119, 0.0112, 0.0111, 0.0099, 0.0099]|\n",
      "|18   |[hope, north, aussie, road, korea]          |[0.0181, 0.0176, 0.0157, 0.0132, 0.0107]|\n",
      "|19   |[hospital, right, federal, boost, warned]   |[0.0253, 0.0103, 0.0090, 0.0088, 0.0088]|\n",
      "|20   |[year, three, alleged, darwin, five]        |[0.0351, 0.0205, 0.0175, 0.0138, 0.0133]|\n",
      "|21   |[four, blast, care, royal, commission]      |[0.0131, 0.0106, 0.0103, 0.0103, 0.0099]|\n",
      "|22   |[country, hour, action, chief, country hour]|[0.0199, 0.0156, 0.0148, 0.0116, 0.0110]|\n",
      "|23   |[minister, health, change, urged, probe]    |[0.0209, 0.0206, 0.0196, 0.0154, 0.0153]|\n",
      "|24   |[park, security, denies, tiger, policy]     |[0.0170, 0.0138, 0.0129, 0.0125, 0.0114]|\n",
      "|25   |[weather, japan, sport, export, tourist]    |[0.0136, 0.0109, 0.0094, 0.0088, 0.0084]|\n",
      "|26   |[green, talk, group, future, question]      |[0.0215, 0.0193, 0.0173, 0.0167, 0.0133]|\n",
      "|27   |[found, missing, body, week, pakistan]      |[0.0223, 0.0180, 0.0162, 0.0123, 0.0115]|\n",
      "|28   |[appeal, close, job, family, vote]          |[0.0121, 0.0110, 0.0104, 0.0102, 0.0102]|\n",
      "|29   |[warning, tour, anti, flood, drought]       |[0.0142, 0.0134, 0.0120, 0.0120, 0.0119]|\n",
      "|30   |[hill, project, broken, clean, union]       |[0.0135, 0.0111, 0.0077, 0.0073, 0.0070]|\n",
      "+-----+--------------------------------------------+----------------------------------------+\n",
      "\n",
      "Topics: 30 Vocabulary: 2\n"
     ]
    }
   ],
   "source": [
    "# Print topics and top-weighted terms\n",
    "numTopics = 30  # cantidad de topicos a explorar\n",
    "topics = ldaModel.describeTopics(maxTermsPerTopic=5)\n",
    "ListOfIndexToWords = udf(lambda wl: list([vocabulario[w] for w in wl]))\n",
    "FormatNumbers = udf(lambda nl: [\"{:1.4f}\".format(x) for x in nl])\n",
    "\n",
    "toptopics = topics.select((topics.topic + 1).alias('topic'),\n",
    "                          ListOfIndexToWords(topics.termIndices).alias('words'),\n",
    "                          FormatNumbers(topics.termWeights).alias('weights'))\n",
    "toptopics.show(truncate=False, n=numTopics)\n",
    "print('Topics:', numTopics, 'Vocabulary:', len(vocabArray))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primeros pasos para la visualización con pyLDAVis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csavGIId53Pc"
   },
   "source": [
    "[pista](https://stackoverflow.com/questions/41819761/pyldavis-visualization-of-pyspark-generated-lda-model)\n",
    "\n",
    "ef prepare(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency):\n",
    "   \"\"\"Transforms the topic model distributions and related corpus data into\n",
    "   the data structures needed for the visualization.\n",
    "    Parameters\n",
    "    ----------\n",
    "    topic_term_dists : array-like, shape (n_topics, n_terms)\n",
    "        Matrix of topic-term probabilities. Where n_terms is len(vocab).\n",
    "    doc_topic_dists : array-like, shape (n_docs, n_topics)\n",
    "        Matrix of document-topic probabilities.\n",
    "    doc_lengths : array-like, shape n_docs\n",
    "        The length of each document, i.e. the number of words in each document.\n",
    "        The order of the numbers should be consistent with the ordering of the\n",
    "        docs in doc_topic_dists.\n",
    "    vocab : array-like, shape n_terms\n",
    "        List of all the words in the corpus used to train the model.\n",
    "    term_frequency : array-like, shape n_terms\n",
    "        The count of each particular term over the entire corpus. The ordering\n",
    "        of these counts should correspond with `vocab` and topic_term_dists.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anaisabel/miniconda3/envs/pyspark_env/lib/python3.9/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(36584, 30)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_term_dists=ldaModel.topicsMatrix().toArray()\n",
    "print(type(topic_term_dists))\n",
    "topic_term_dists.shape"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "1NMSndtR057_",
    "D0vVi8o6_aRS"
   ],
   "name": "LDA con pySpark.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
